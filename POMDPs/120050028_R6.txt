Rohit Kumar
120050028

Primary reference : Planning and acting in partially observable stochastic domains; Leslie Pack Kaelbling, Michael L. Littman, Anthony R. Cassandra

The paper talks about partial observable stochastic domains. In many situations like robot navigation, the agent is unable to determine its current state with complete reliability. It only achieves partial information about the state it is currently in and based on that it has to decide the proper action to maximize its expected future reward. 

The naive approach to this for the agent can be to map the most recent observation of the state directly into an action without remembering past consequences for the same. This approach leads to performing the same action for every state whose observation is same. Other approaches which use current observation only (e.g. adding randomness to observation) can't be truly effective ever and there is a necessity to use memory from previous actions and observations to add clarity of the states.

The formulation of POMDPs build upon MDPs in which where states are clearly known. Apart from the states, actions, transition function and reward function, POMDPs consist of a finite set of observations that the agent can experience from the environment and a probability distribution over the observation space for each action and resulting state pair. The agent's goal remains the same i.e. to maximize the discounted future reward. 

The paper decomposes the problem of controlling a POMDP into two parts. It introduces a new entity called belief state which summarizes the previous experiences of the agent. A new component called state estimator is responsible of updating it based on the last action, current observation and previous belief state. The policy which generates actions is modified to take this belief state as input rather than the original state of the environment. The belief state can be formulated in many ways. In the paper it is described as a probability distribution over the original states of the world. The formulation/encoding ensures that they comprise of a sufficient statistic of the past history. If the belief state is provided, no additional data about its past actions or observations would provide any further information which implies that the process executed over belief states is Markovian in nature.

The probability distribution for belief state is computed using Bayes theorem and basic probability concepts using previous probability distribution (belief state). For calculating the optimal policy, the new state space is made of belief states (which is a continuous space), the action set remains the same, a new transition function is defined which considers transition from one belief state to other for a particular action. The reward function is constructed from the original reward function using the probability distribution of belief state. This belief MDP is made such that an optimal policy along with the correct state estimator (belief state) will give optimal behavior.

Solving a general continuous space MDP is difficult in general, but the optimal value function for the belief MDP has some special properties which has been exploited in the paper. For calculating the value function, value iteration is used over belief space. It is approximated by the optimal t-step discounted value function which is represented by a policy tree. The top node determines the first action to be taken and eventually depending upon the observation, a proper branch of the tree is chosen which determines the next action and so on. Several optimizations over this have been suggested, like the witness algorithm. Finally the paper talks about how to interpret the policy obtained, both for finite and infinite horizons.

Overall the paper provides a good start to the world of partial observability and suggests a lot of ideas which can be taken further to built upon. There can be a lot of extensions e.g. using hidden Markov models etc. The approaches focus both on performing actions taken to gain information from the environment as well as to fetch rewards.