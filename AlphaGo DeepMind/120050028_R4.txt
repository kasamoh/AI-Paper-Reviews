Rohit Kumar
120050028

Primary reference : Mastering the game of Go with deep neural networks and tree search, Google

The paper describes a program called AlphaGo which achieves a 99.8% winning rate against all other Go programs. This program has beaten the human European Go champion 5 games to 0. It was believed that it would take a decade to achieve this feat.

In large games like chess and Go, an exhaustive search over all possible sequences of moves is in-feasible because of large breadth and depth (~250 and ~150 respectively in Go). There are two general principles to reduce the search space, the first one being truncating the search tree at a particular state and replacing the sub-tree below by an approximate value function. This approach makes superhuman performance achievable in games like chess, checkers and othello. The second approach is to reduce the breadth of the search tree by sampling actions from a policy which a probability distribution over the possible actions at a particular state. Examples include Monte Carlo roll-outs which search to maximum depth without branching at all. In this process sampling of long sequences of actions for both players are performed and by averaging over them, an effective policy is formulated which leads to superhuman performance in games such as backgammon and scrabble.

Prior to the paper, the basis of work in this area has been towards linear combination of input features. With the advent of deep convolution neural networks, many unprecedented performances have been achieved. The paper uses these to reduce the effective depth and breadth of the search tree and proposes two networks, the value network to evaluate states and the policy network to sample actions from a particular state. The neural networks are trained using a pipeline model in which machine learning is applied at several stages. Initially the policy network is trained using supervised learning taking data directly from expert human moves. Then this is improved using RL which uses the outcome of self-play. The program AlphaGo efficiently combines the policy and value networks with Monte Carlo Tree Search.

The SL policy network alternates between convolution layers and rectifiers (a non-linear activation function). The final layer is a soft-max layer which outputs a probability distribution over all legal moves. The input to this network is a simple representation of the board. The second stage is improving the network by policy gradient RL. Games are played between current network and a randomly selected previous iteration of the network. It has been stated that this randomization avoids over-fitting of the current policy. A reward function which is 0 at non-terminal states and 1(winning) or -1(losing) at terminal states. Weights are updated at each step by stochastic gradient ascent in the direction that maximizes expected outcome. This improved policy wins around 80\% of the games played against the initial SL based policy.

The final stage is position evaluation which estimates the value function i.e. the outcome from a particular state. This is done using the value network which a neural network similar to the policy network except that it outputs a single value rather than a probability distribution.

AlphaGo combines these two policies with MCTS that selects actions through look-ahead search. Each edge stores an action value(Q value). visit count and a prior probability. Simulation is run and at each time step, that action is selected which maximizes an appropriate mix of Q value and prior probability per visit. At the end, the Q values and visit counts are updated. Several evaluations have been discussed in which AlphaGo plays with various renowned programs for Go.

Overall, the paper gives a good comprehensive study of the program and based on the evaluations that the program goes through, it really seems to be a breakthrough in AI. Although for the complete understanding of the paper, one has to know in advance concepts such as CNNs, rectifier non-linearity etc.
